{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc67d11c",
   "metadata": {},
   "source": [
    "## LLM Workshop\n",
    "By: Mohammed Alageel\n",
    "\\\n",
    "Prerequisites:\n",
    "- Basic understanding of Python programming concepts.\n",
    "- A laptop equipped with::\n",
    "    - Python (version 3.11 is recommended).\n",
    "    - An IDE like VS Code or PyCharm.\n",
    "    - Ollama installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0199df1a",
   "metadata": {},
   "source": [
    "# 1. What Are LLMs?\n",
    "Large Language Models (LLMs) are AI models trained on massive amounts of text to understand and generate human-like language.\n",
    "\n",
    "## Key Concepts\n",
    "- Based on transformer architecture (e.g., GPT, BERT).\n",
    "- Trained to predict the next word (or token) in a sequence.\n",
    "- Can generate, translate, summarize, and answer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddaccf5",
   "metadata": {},
   "source": [
    "## Tokens\n",
    "LLMs process text by breaking it down into smaller units called **tokens**.\\\n",
    "A token can represent a whole word, a part of a word (sub-word), or even a single character.\\\n",
    "![Tokens](images/tokenizer.png)\\\n",
    "You can explore how text is tokenized using tools like the [OpenAI Tokenizer](https://platform.openai.com/tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f77523f",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "The complexity and capability of an LLM are often related to its number of **parameters**. These are the internal variables the model learns during training.\\\n",
    "Model Parameter size ranges from 1B (Small) to 100B+ (Large)\\\n",
    "Generally, models with more parameters have greater capacity, but also require more computational resources.\\\n",
    "\\\n",
    "![Parameters](images/model_size.png)\\\n",
    "[Ollama Gemma3 Model](https://ollama.com/library/gemma3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e41da3",
   "metadata": {},
   "source": [
    "## Context Length\n",
    "The **context length** defines the maximum amount of text (measured in tokens) that an LLM can consider at one time when processing input or generating output.\\\n",
    "This limit varies between different models, typically ranging from a few thousand (e.g., 4k) to over a hundred thousand (e.g., 128k) tokens.\\\n",
    "![Context](images/context_length.png)\\\n",
    "[HuggingFace LLama4 Link](https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac7f83d",
   "metadata": {},
   "source": [
    "## Popular LLMs\n",
    "- ChatGPT (OpenAI)\n",
    "- Claude (Anthropic)\n",
    "- Gemini (Google)\n",
    "- LLaMA (Meta)\n",
    "- Qwen (Alibaba)\n",
    "\n",
    "## Common Use Cases\n",
    "- Writing assistance (emails, blog posts)\n",
    "- Code generation\n",
    "- Chatbots & virtual assistants\n",
    "- Language translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1258cd8d",
   "metadata": {},
   "source": [
    "# 2. Basic LLM Usage\n",
    "How do you use an LLM?\\\n",
    "You give it a prompt (a message or instruction), and it returns a response.\n",
    "\n",
    "## Examples\n",
    "Prompt: “Write a birthday message for a 10-year-old.”\\\n",
    "Output: “Happy 10th Birthday! Hope your day is filled with fun and cake!”\n",
    "\n",
    "Prompt: “What’s the capital of France?”\\\n",
    "Output: “Paris.\n",
    "\n",
    "## APIs\n",
    "- Low-level: Transformers\n",
    "- High-level: OpenAI (Ollama or Cloud)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ce33d787c5eda8",
   "metadata": {},
   "source": [
    "## Low level: Transformers\n",
    "Offer fine-grained control (e.g., Hugging Face `transformers`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759ed5be12f44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500111281de61f98",
   "metadata": {},
   "source": [
    "## High level: OpenAI\n",
    "Provide simpler interfaces (e.g., OpenAI's API, usable with cloud services or local tools like Ollama)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key='...',\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What’s the capital of France?\"},\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357b4f4698a56312",
   "metadata": {},
   "source": [
    "## Ollama\n",
    "Ollama provides a convenient way to run various open-source LLMs directly on your own machine.\\\n",
    "It exposes an API endpoint compatible with the OpenAI API standard, allowing you to use the same `openai` Python library to interact with local models.\\\n",
    "**Terminal Commands**:\n",
    "```bash\n",
    "ollama run qwen2.5:1.5b\n",
    "```\n",
    "To run a larger version (e.g., Qwen 7B):\n",
    "\n",
    "```bash\n",
    "ollama run qwen2.5:7b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d714547c38e7c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen2.5:1.5b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What’s the capital of France?\"},\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6243267f0bfba6b",
   "metadata": {},
   "source": [
    "## LLM Hyperparameters\n",
    "When generating text, you can influence the output using several parameters:\n",
    "- **`max_tokens`**: Sets the maximum number of tokens the model should generate in its response.\n",
    "- **`temperature`**: Controls the randomness of the output. A lower value (e.g., 0) makes the output more deterministic and focused (greedy decoding), while a higher value increases randomness and creativity.\n",
    "- **`top_p` (Nucleus Sampling)**: Selects tokens from a cumulative probability distribution. Only the most probable tokens whose probabilities add up to `top_p` are considered. `top_p=1` considers all tokens, while lower values restrict choices.\n",
    "- **`top_k`**: Selects only the `k` most likely tokens at each step. `top_k=1` is equivalent to greedy decoding.\n",
    "                \n",
    "**Warning:** Setting `temperature` very high without constraints like `max_tokens` can sometimes lead to repetitive or nonsensical output loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c218aa38b2d4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen2.5:1.5b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What’s the capital of France?\"},\n",
    "    ],\n",
    "    max_tokens=100, # if not used we will enter an infinite loop\n",
    "    temperature=5,\n",
    "    top_p=1,\n",
    "    # top_k is not supported here\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89720ae5",
   "metadata": {},
   "source": [
    "## Hallucination\n",
    "<!-- LLMs may generate incorrect information confidently, which is referred to as hallucination.\\\n",
    "To reduce its impact, we can ask the LLM to only generate an answer if it knows the answer.\\\n",
    "If it does not know the answer, it can respond with\n",
    "“I don’t know.” -->\n",
    "LLMs can sometimes generate text that sounds plausible but is factually incorrect or nonsensical. This phenomenon is known as **hallucination**.\n",
    "\n",
    "It occurs because models predict likely sequences of words based on patterns in their training data, without true understanding or access to real-time facts.\n",
    "\n",
    "One way to mitigate this is to instruct the model in the prompt to state when it doesn't know an answer, for example:\n",
    "```python\n",
    "\"Answer the following question. If you do not know the answer or cannot find it in the provided context, respond with 'I don’t know.'\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579045aa1052f83f",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "**Tokenization** is the fundamental process of converting raw text into a sequence of tokens (numerical IDs) that the model can understand.\n",
    "\n",
    "The specific way text is broken down depends on the **tokenizer** used, which is typically paired with a specific LLM.\n",
    "![tokenizer](images/tokenizer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32943f385a07d40f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T06:04:13.145482Z",
     "start_time": "2025-04-21T06:04:02.505965Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "prompt = \"Hey, are you conscious? Can you talk to me?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "print(inputs)\n",
    "\n",
    "print(inputs.input_ids[0][0], tokenizer.decode(inputs.input_ids[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e2f278a7b5141a",
   "metadata": {},
   "source": [
    "# 3. Prompt Engineering\n",
    "Prompt Engineering is the craft of designing effective inputs to get useful outputs from an LLM.\n",
    "\n",
    "## Techniques\n",
    "1. Zero-shot: Ask directly.\n",
    "```python\n",
    "\"Summarize this article. [ARTICLE]\"\n",
    "```\n",
    "2. Few-shot: Give examples.\n",
    "```python\n",
    "\"\"\"Classify as Negative or Positive\n",
    "example:\n",
    "this drink made me vomit\n",
    "output:\n",
    "negative\n",
    "\n",
    "[INPUT]\n",
    "output:\n",
    "\"\"\"\n",
    "```\n",
    "3. Chain-of-Thought: Ask for reasoning.\n",
    "```python\n",
    "\"When I was 3 years old, my partner was 3 times my age. Now, I am 20 years old. How old is my partner?\"\n",
    "```\n",
    "4. Role prompting:\n",
    "```python\n",
    "\"Act like a legal advisor and explain this contract. [CONTRACT]\"\n",
    "```\n",
    "\n",
    "## Tips\n",
    "- provide examples.\n",
    "- Be specific about the output.\n",
    "- Use instructions over constraints.\n",
    "- Experiment and iterate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3a6d9846ced59",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T06:16:51.033268Z",
     "start_time": "2025-04-21T06:16:48.331380Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "customer_order = \"Now, I would like a large pizza, with the first half cheese and mozzarella. And the other tomato sauce, ham and pineapple.\"\n",
    "prompt = \"\"\"\n",
    "Parse a customer's pizza order into valid JSON:\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [[\"cheese\", \"tomato sauce\", \"peperoni\"]]\n",
    "}\n",
    "```\"\"\" + \"\\n\" + customer_order + \"\\nJSON Response:\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen2.5:1.5b\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    "    temperature=0,\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b876327",
   "metadata": {},
   "source": [
    "## Parsing Structured Output\n",
    "When you need the LLM's output to be used programmatically (e.g., feeding data into another system), it's crucial to get structured data like JSON.\n",
    "\n",
    "You can prompt the model to generate JSON. However, the output might not always be perfectly valid.\n",
    "\n",
    "Libraries like **`Pydantic`** are excellent for:\n",
    "1.  Defining the expected data structure using Python classes.\n",
    "2.  Parsing the LLM's JSON output.\n",
    "3.  Validating that the parsed data conforms to the defined structure.\n",
    "\n",
    "\n",
    "**JSON Repair Libraries:** Tools like **`json-repair`** can attempt to automatically fix common errors in malformed JSON strings before parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1b3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class Order(BaseModel):\n",
    "    size: str\n",
    "    type: str\n",
    "    ingredients: list[list[str]]\n",
    "\n",
    "json_str = response.choices[0].message.content.lstrip('```json').rstrip('```')\n",
    "print(json_str)\n",
    "order = Order.model_validate_json(json_str)\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7775a2dfda237508",
   "metadata": {},
   "source": [
    "# 4. Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "**RAG** stands for **Retrieval-Augmented Generation**. It's a powerful technique that enhances LLM responses by providing them with relevant information retrieved from an external knowledge source (like a collection of documents or a database) before generation.\n",
    "\n",
    "This helps produce more accurate, up-to-date, and context-aware answers, especially for domain-specific or recent information not present in the LLM's original training data.\n",
    "\n",
    "\n",
    "## Text Embeddings\n",
    "A core component of RAG is **text embedding**. This process converts pieces of text into numerical vectors (lists of numbers).\n",
    "\n",
    "These vectors are designed to capture the semantic meaning of the text, such that texts with similar meanings have vectors that are close to each other in the vector space.\n",
    "\n",
    "![embeddings](images/emb.png)\n",
    "\n",
    "### Usages\n",
    "- Semantic Search\n",
    "- Recommendation Systems\n",
    "- Text Classification\n",
    "- Text Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91eca41759e329",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T06:18:05.321951Z",
     "start_time": "2025-04-21T06:18:00.984468Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "# Load model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "# Convert text to text embeddings\n",
    "vector = model.encode(\"Best movie ever!\")\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8612b211d57e109",
   "metadata": {},
   "source": [
    "## Vector DB\n",
    "**Vector databases** are specialized databases designed to efficiently store and search through large collections of embedding vectors.\n",
    "\n",
    "Their key capability is performing **similarity searches**. Given a query vector (representing a question or topic), the database can quickly find the stored vectors (representing documents or data chunks) that are most similar in meaning.\n",
    "\n",
    "![vector2](images/vector_search.png)\n",
    "\n",
    "### Typical RAG Indexing and Querying Workflow\n",
    "1. All Text → Embed (e.g., via SentenceTransformers, OpenAI)\n",
    "2. Store embeddings in vector DB (like FAISS, ChromaDB, Pinecone, Weaviate)\n",
    "3. Query with new text → Embed\n",
    "4. Use Query in vector DB -> get most similar documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e368beb1",
   "metadata": {},
   "source": [
    "## Knowledge cutoffs\n",
    "LLMs are trained on data up to a certain point in time (their **knowledge cutoff**). They typically lack information about events or developments occurring after that date.\n",
    "\n",
    "RAG is an effective solution to this limitation, as it allows the model to access and incorporate current information retrieved from an up-to-date external knowledge source during the generation process.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e97f474f2d88a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T06:33:21.071896Z",
     "start_time": "2025-04-21T06:33:18.332892Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "from typing import List, Callable\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "def make_index(texts: List[str], embedding_model: Callable[[List[str]], np.ndarray]) -> faiss.Index:\n",
    "    # Generate embeddings for all texts\n",
    "    embeddings = embedding_model(texts)\n",
    "    # Get dimensionality from the embeddings\n",
    "    dimension = embeddings.shape[1]\n",
    "    # Create a FAISS index - using L2 distance (Euclidean)\n",
    "    index = faiss.IndexFlatL2(dimension)\n",
    "    # Add embeddings to the index\n",
    "    index.add(embeddings.astype(np.float32))\n",
    "    return index\n",
    "\n",
    "def make_query(query: str, embedding_model: Callable[[List[str]], np.ndarray],\n",
    "               index: faiss.Index, k: int = 5) -> np.ndarray:\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedding_model([query])\n",
    "    # Search the index\n",
    "    _, indices = index.search(query_embedding.astype(np.float32), k)\n",
    "    # Return the indices\n",
    "    return indices[0]  # Return the first (and only) result's indices\n",
    "\n",
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"Machine learning is a subset of artificial intelligence\",\n",
    "    \"Python is a popular programming language for data science\",\n",
    "    \"Neural networks have revolutionized natural language processing\",\n",
    "    \"FAISS is a library for efficient similarity search\",\n",
    "    \"Vector databases store and retrieve embeddings efficiently\",\n",
    "    \"Climate change is affecting global weather patterns\",\n",
    "    \"Renewable energy sources include solar and wind power\",\n",
    "    \"Electric vehicles are becoming increasingly popular\",\n",
    "    \"Quantum computing uses quantum bits or qubits\",\n",
    "    \"Blockchain technology enables secure decentralized transactions\",\n",
    "    \"Healthy eating involves consuming a balanced diet\",\n",
    "    \"Regular exercise improves physical and mental health\",\n",
    "    \"Space exploration has led to many technological advances\",\n",
    "    \"The Great Barrier Reef is the world's largest coral reef system\",\n",
    "    \"Digital transformation is changing how businesses operate\",\n",
    "    \"Cybersecurity protects systems from digital attacks\",\n",
    "    \"Artificial intelligence can solve complex problems\",\n",
    "    \"Cloud computing delivers computing services over the internet\",\n",
    "    \"Data privacy concerns are growing in the digital age\"\n",
    "]\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "faiss_index = make_index(texts, model.encode)\n",
    "user_query = \"What is artificial intelligence?\"\n",
    "results = make_query(user_query, model.encode, faiss_index, k=5)\n",
    "\n",
    "for i in results:\n",
    "    print(i, texts[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f78e1bd6db20f10",
   "metadata": {},
   "source": [
    "## RAG Code\n",
    "\n",
    "### How It Works\n",
    "1. User query → embedding\n",
    "2. Search vector DB → find relevant documents\n",
    "3. Combine prompt + documents + query → full LLM prompt\n",
    "4. LLM generates answer using retrieved context\n",
    "\n",
    "### Example\n",
    "1. “What’s in the company’s refund policy?”\n",
    "2. Vector DB retrieves policy snippet.\n",
    "3. LLM answers: “You can request a refund within 30 days...”\n",
    "\\\n",
    "\\\n",
    "![rag](images/rag.png)\n",
    "\n",
    "### Benefits\n",
    "- Reduces hallucination\n",
    "- Up-to-date answers (even beyond model’s training data)\n",
    "- Domain-specific accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b063d040e6b8960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T06:36:00.119871Z",
     "start_time": "2025-04-21T06:35:56.788399Z"
    }
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = 'http://localhost:11434/v1',\n",
    "    api_key='ollama',\n",
    ")\n",
    "\n",
    "system_prompt = \"You are a helpful assistant. You may only answer based on the documents given to you, if you don't know, say i don't know.\"\n",
    "docs = [texts[i] for i in results]\n",
    "docs_text = \"\\n\".join(docs)\n",
    "full_prompt = f\"\"\"{system_prompt}\n",
    "DOCUMENTS:\n",
    "{docs_text}\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"qwen2.5:1.5b\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": full_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "print(full_prompt)\n",
    "print('-----------------')\n",
    "print(user_query)\n",
    "print('-----------------')\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f826214",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "For large documents, embedding the entire text at once can be inefficient and may dilute specific details. A common strategy is **chunking**: splitting the document into smaller, potentially overlapping, segments (chunks).\n",
    "\n",
    "Each chunk is then embedded and stored individually. During retrieval, the system finds the most relevant chunks to provide focused context to the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1362dabb1cedc4",
   "metadata": {},
   "source": [
    "# 5. Finetuning\n",
    "Fine-tuning means training an existing LLM on your custom dataset to specialize it.\n",
    "\n",
    "## When to Use\n",
    "- Need highly specific outputs **(style, tone or format)**\n",
    "- Want to reduce cost\n",
    "- **Note:** Fine-tuning can be complex and resource-intensive. It's often considered after exploring prompt engineering and RAG.\n",
    "\n",
    "## Example\n",
    "A legal firm fine-tunes a model on case law and terminology.\\\n",
    "The LLM now speaks “legalese” better than a generic one.\n",
    "\n",
    "## Tools\n",
    "- OpenAI fine-tuning API (for smaller models)\n",
    "- LoRA / PEFT (Parameter-Efficient Fine-Tuning)\n",
    "- Hugging Face Transformers\n",
    "\n",
    "[Youtube Link for finetuning](https://www.youtube.com/watch?v=S9VHQhC3HPc)\n",
    "\n",
    "## Model Finetuning Example\n",
    "Most models are released with 2 types:\n",
    "- Base Model\n",
    "- Instruct Finetune\\\n",
    "\\\n",
    "![instruct](images/instruct.png)\\\n",
    "Base models are just text completion\\\n",
    "Instruct models are tuned for question answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28beb006",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B-Instruct\")\n",
    "\n",
    "prompt = \"Hey.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=30)\n",
    "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2f9bbd",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "**Quantization** is a technique used to reduce the memory footprint and computational cost of running LLMs.\n",
    "\n",
    "\n",
    "![quant](images/quantization.png)\n",
    "\n",
    "This significantly decreases the model size and can speed up inference, often with only a small impact on performance. It's crucial for running larger models on consumer hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1005ce56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-1.5B\",\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "model_full = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "print(f'Q8 Quantization: {model_8bit.get_memory_footprint() / (1024 * 1024 * 1024):.2f}GB')\n",
    "print(f'Full Precision: {model_full.get_memory_footprint() / (1024 * 1024 * 1024):.2f}GB')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea7274a",
   "metadata": {},
   "source": [
    "\n",
    "## Special Tokens and Chat Templates\",\n",
    "LLMs and their tokenizers use **special tokens** – unique symbols that don't represent regular words but serve structural or functional purposes.\n",
    "\n",
    "Examples include:\n",
    "- `[CLS]`, `<s>`: Mark the beginning of a sequence.\n",
    "- `[SEP]`, `</s>`: Indicate separation between segments or the end of a sequence.\n",
    "- `[PAD]`: Used to pad shorter sequences to a uniform length in a batch.\n",
    "- `[UNK]`: Represents tokens that were not in the tokenizer's vocabulary.\n",
    "\n",
    "For chat models, specific tokens and formatting rules (**chat templates**) are used to delineate between system messages, user turns, and assistant turns. Applying the correct chat template is crucial for getting instruct/chat models to behave as expected.\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a229e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-1.5B\")\n",
    "\n",
    "chat = [\n",
    "  {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, # Try commenting this\n",
    "  {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n",
    "]\n",
    "\n",
    "tokenizer.apply_chat_template(chat, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd3f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7aba823b73be7",
   "metadata": {},
   "source": [
    "# 6. Agents\n",
    "LLM agents use an LLM as a thinking brain that plans, decides, and interacts with tools or APIs to complete tasks.\n",
    "\n",
    "![agents](images/agentic-ai-workflow.png)\n",
    "\n",
    "## Components\n",
    "- **LLM**: reasoning engine\n",
    "- **Tools**: external APIs, databases, calculators\n",
    "- **Memory**: stores past interactions\n",
    "- **Planner/Executor**: decides what to do next\n",
    "\n",
    "## Example\n",
    "An AI assistant that:\n",
    "1. Receives a task: “Book me a flight to NY.”\n",
    "2. Calls flight APIs.\n",
    "3. Picks the cheapest option.\n",
    "4. Sends a confirmation email.\n",
    "\n",
    "## Security Warning\n",
    "Granting LLM agents the ability to execute actions (especially interacting with **file systems**, **databases**, or **external APIs**)\n",
    "\n",
    "introduces significant **security risks**. The agent could be manipulated (via malicious prompts) into performing harmful actions.\n",
    "\n",
    "Careful design, sandboxing, and validation of tool inputs/outputs are crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d67e937459157a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import os\n",
    "\n",
    "\n",
    "class Message:\n",
    "    \"\"\"Represents a message in the conversation.\"\"\"\n",
    "    def __init__(self, role: str, content: str):\n",
    "        self.role = role\n",
    "        self.content = content\n",
    "\n",
    "    def to_dict(self) -> Dict[str, str]:\n",
    "        \"\"\"Convert message to dictionary format for OpenAI API.\"\"\"\n",
    "        return {\n",
    "            \"role\": self.role,\n",
    "            \"content\": self.content\n",
    "        }\n",
    "\n",
    "\n",
    "class ChatHistory:\n",
    "    \"\"\"Manages conversation history.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.messages: List[Message] = []\n",
    "\n",
    "    def add_message(self, role: str, content: str) -> None:\n",
    "        \"\"\"Add a new message to the history.\"\"\"\n",
    "        self.messages.append(Message(role, content))\n",
    "\n",
    "    def get_history_for_prompt(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"Get formatted history for use in LLM prompts.\"\"\"\n",
    "        return [message.to_dict() for message in self.messages]\n",
    "\n",
    "\n",
    "def generate_response(client: OpenAI, messages: List[Dict[str, str]]) -> str:\n",
    "    \"\"\"Generate a response from the language model.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"qwen2.5:7b\",\n",
    "        messages=messages,\n",
    "        temperature=0.3\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Base class for all agents.\"\"\"\n",
    "    def __init__(self, name: str, system_prompt: str, client: OpenAI):\n",
    "        self.name = name\n",
    "        self.system_prompt = system_prompt\n",
    "        self.client = client\n",
    "\n",
    "    def process(self, user_input: str, chat_history: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Process a user request and return a response.\"\"\"\n",
    "        # implemented by subclass\n",
    "        ...\n",
    "\n",
    "\n",
    "class AgentResponse(BaseModel):\n",
    "    timestamp: str\n",
    "    name: str\n",
    "    input: str\n",
    "    output: str\n",
    "    \n",
    "    def to_dict(self) -> dict[str, str]:\n",
    "        \"\"\"convert agent response to dictionary format for OpenAI API.\"\"\"\n",
    "        return {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": f\"{self.timestamp} - agent={self.name}: input={self.input}\\n\\noutput={self.output}\"\n",
    "        }\n",
    "\n",
    "\n",
    "class FileAgent(Agent):\n",
    "    \"\"\"Agent specializing in file operations.\"\"\"\n",
    "    def __init__(self, client: OpenAI):\n",
    "        super().__init__(\n",
    "            name=\"FileAgent\",\n",
    "            system_prompt=\"\"\"You are a file operations agent. Your job is to \n",
    "1. include the filename in the first line\n",
    "2. give the input to write to the file\n",
    "example:\n",
    "write xyz to a file\n",
    "output:\n",
    "file.txt\n",
    "xyz\"\"\",\n",
    "            client=client\n",
    "        )\n",
    "    \n",
    "    def process(self, user_input: str, chat_history: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Process a file-related request and return instructions.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ] + chat_history\n",
    "        resp = generate_response(self.client, messages)\n",
    "        filename = resp.split('\\n')[0]\n",
    "        text = \"\\n\".join(resp.split('\\n')[1:])\n",
    "        if os.path.isfile(filename):\n",
    "            return f\"Couldn't write to {filename}, there is a file\"\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(text)\n",
    "        return f\"Wrote to {filename}\"\n",
    "\n",
    "\n",
    "class DatabaseAgent(Agent):\n",
    "    \"\"\"Agent specializing in querying and retrieving data.\"\"\"\n",
    "    def __init__(self, client: OpenAI):\n",
    "        super().__init__(\n",
    "            name=\"DatabaseReadAgent\",\n",
    "            system_prompt=\"\"\"You are a database query agent. Your job is to:\n",
    "            1. Interpret the user's data retrieval needs\n",
    "            2. Formulate appropriate database queries (SQL or other query language)\n",
    "            3. Explain how to retrieve the requested information\n",
    "            Be precise in your query syntax and explain the expected results.\n",
    "            \"\"\",\n",
    "            client=client\n",
    "        )\n",
    "    \n",
    "    def process(self, user_input: str, chat_history: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Process a database query request and return instructions.\"\"\"\n",
    "        db = [\n",
    "                \"The quick brown fox jumps over the lazy dog\",\n",
    "                \"Machine learning is a subset of artificial intelligence\",\n",
    "                \"Python is a popular programming language for data science\",\n",
    "                \"Neural networks have revolutionized natural language processing\",\n",
    "                \"FAISS is a library for efficient similarity search\",\n",
    "                \"Vector databases store and retrieve embeddings efficiently\",\n",
    "                \"Climate change is affecting global weather patterns\",\n",
    "                \"Renewable energy sources include solar and wind power\",\n",
    "                \"Electric vehicles are becoming increasingly popular\",\n",
    "                \"Quantum computing uses quantum bits or qubits\",\n",
    "                \"Blockchain technology enables secure decentralized transactions\",\n",
    "                \"Healthy eating involves consuming a balanced diet\",\n",
    "                \"Regular exercise improves physical and mental health\",\n",
    "                \"Space exploration has led to many technological advances\",\n",
    "                \"The Great Barrier Reef is the world's largest coral reef system\",\n",
    "                \"Digital transformation is changing how businesses operate\",\n",
    "                \"Cybersecurity protects systems from digital attacks\",\n",
    "                \"Artificial intelligence can solve complex problems\",\n",
    "                \"Cloud computing delivers computing services over the internet\",\n",
    "                \"Data privacy concerns are growing in the digital age\"\n",
    "        ]\n",
    "        return \"\\n\".join(db)\n",
    "\n",
    "\n",
    "class FinalAnswerAgent(Agent):\n",
    "    \"\"\"Agent for providing final answers to the user.\"\"\"\n",
    "    def __init__(self, client: OpenAI):\n",
    "        super().__init__(\n",
    "            name=\"FinalAnswerAgent\",\n",
    "            system_prompt=\"\"\"You are the final answer agent. Your job is to:\n",
    "            1. Synthesize information from previous agent interactions\n",
    "            2. Provide a clear, helpful response to the user's query\n",
    "            3. Be concise but thorough in your explanations\n",
    "            If you don't have enough information, say so clearly.\n",
    "            \"\"\",\n",
    "            client=client\n",
    "        )\n",
    "    \n",
    "    def process(self, user_input: str, chat_history: List[Dict[str, str]]) -> str:\n",
    "        \"\"\"Process the request and provide a final answer to the user.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ] + chat_history\n",
    "        return generate_response(self.client, messages)\n",
    "\n",
    "\n",
    "class RouterAgent(Agent):\n",
    "    \"\"\"Meta-agent that routes requests to specialized agents.\"\"\"\n",
    "    def __init__(self, client: OpenAI):\n",
    "        self.agents_memory = []\n",
    "        self.chat_history = ChatHistory()\n",
    "        super().__init__(\n",
    "            name=\"RouterAgent\",\n",
    "            system_prompt=\"\"\"You are a teacher agent that determines which specialized agent should handle a user's request.\n",
    "Respond only with the name of the agent that should handle this request from the following options:\n",
    "Please only answer with information from the database, if you don't know say you don't know\n",
    "- FileAgent: For writing files\n",
    "- DatabaseAgent: For querying and retrieving data\n",
    "- FinalAnswerAgent: For final answer to user\"\"\",\n",
    "            client=client\n",
    "        )\n",
    "\n",
    "        # Initialize agents\n",
    "        self.file_agent = FileAgent(client)\n",
    "        self.database_agent = DatabaseAgent(client)\n",
    "        self.final_answer_agent = FinalAnswerAgent(client)\n",
    "        \n",
    "        # Mapping of agent names to agent instances\n",
    "        self.agents = {\n",
    "            \"fileagent\": self.file_agent,\n",
    "            \"databaseagent\": self.database_agent,\n",
    "            \"finalansweragent\": self.final_answer_agent\n",
    "        }\n",
    "    \n",
    "    def process(self, user_input, chat_history):\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": self.system_prompt}\n",
    "        ] + chat_history\n",
    "        return generate_response(self.client, messages)\n",
    "\n",
    "    def route_request(self, user_input: str, chat_history: List[Dict[str, str]]) -> Tuple[str, str]:\n",
    "        \"\"\"Route the user request to the appropriate agent.\"\"\"\n",
    "        # Ask the teacher agent which specialized agent should handle this\n",
    "        routing_decision = self.process(user_input, chat_history).lower()\n",
    "\n",
    "        # Parse the routing decision to get the agent name\n",
    "        selected_agent = None\n",
    "        for agent_name, agent in self.agents.items():\n",
    "            if agent_name in routing_decision:\n",
    "                selected_agent = agent\n",
    "                break\n",
    "\n",
    "        # Default to general query if no specific agent is identified\n",
    "        if not selected_agent:\n",
    "            selected_agent = self.final_answer_agent\n",
    "\n",
    "        # Get response from the selected agent\n",
    "        response = selected_agent.process(user_input, chat_history)\n",
    "        \n",
    "        return agent_name, response\n",
    "\n",
    "    def process_input(self, user_input: str) -> str:\n",
    "        \"\"\"Process user input through the agent system.\"\"\"\n",
    "        # Add user message to chat history\n",
    "        self.chat_history.add_message(\"user\", user_input)\n",
    "        agent = None\n",
    "        max_tries = 3\n",
    "        while max_tries >= 0 and (agent is None or agent.lower() != 'finalansweragent'):\n",
    "            max_tries -= 1          \n",
    "            # Route the request to the appropriate agent\n",
    "            agent, response = self.route_request(\n",
    "                user_input,\n",
    "                self.chat_history.get_history_for_prompt() + [m.to_dict() for m in self.agents_memory]\n",
    "            )\n",
    "            if agent.lower() != 'finalansweragent':\n",
    "                agent_response = AgentResponse(\n",
    "                    timestamp=str(datetime.now()),\n",
    "                    name=agent,\n",
    "                    input=user_input,\n",
    "                    output=response\n",
    "                )\n",
    "                self.agents_memory.append(agent_response)\n",
    "\n",
    "        # Add agent response to chat history\n",
    "        self.chat_history.add_message(\"assistant\", response)\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "agent_system = RouterAgent(\n",
    "    client=OpenAI(  \n",
    "        base_url = 'http://localhost:11434/v1',\n",
    "        api_key='ollama', # required, but unused\n",
    "    )\n",
    ")\n",
    "\n",
    "# Example interactions\n",
    "# query = \"Tell me what information you have about artificial intelligence.\"\n",
    "query = \"Create a file summarizing what you know about climate and energy.\"\n",
    "print(f\"User: {query}\")\n",
    "response = agent_system.process_input(query)\n",
    "print(f\"System: {response}\")\n",
    "print(\"=\"*50)\n",
    "print(\"Agents memories:\")\n",
    "for a in agent_system.agents_memory:\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee3f5a0",
   "metadata": {},
   "source": [
    "# Thanks for listening"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
